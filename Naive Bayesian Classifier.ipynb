{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d2336a8",
   "metadata": {},
   "source": [
    "# C4.5 Decision Tree Algorithm - Employee Database Analysis\n",
    "\n",
    "## Problem Statement\n",
    "Given the employee database training data, we need to determine which attribute should be selected to split the records in the first iteration using the C4.5 decision tree algorithm with Gain Ratio as the uncertainty measure.\n",
    "\n",
    "## Dataset\n",
    "The training data consists of 11 records with attributes:\n",
    "- **department**: sales, systems, marketing, secretary\n",
    "- **status**: senior, junior  \n",
    "- **age**: 21-30, 31-40, 41-50\n",
    "- **salary** (class attribute): Low, Medium, High\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4909803f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b481c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   department  status    age  salary\n",
      "0       sales  senior  31-40  Medium\n",
      "1       sales  junior  21-30     Low\n",
      "2       sales  junior  31-40     Low\n",
      "3     systems  junior  21-30  Medium\n",
      "4     systems  senior  31-40    High\n",
      "5     systems  junior  21-30  Medium\n",
      "6     systems  senior  41-50    High\n",
      "7   marketing  senior  31-40  Medium\n",
      "8   marketing  junior  31-40  Medium\n",
      "9   secretary  senior  41-50  Medium\n",
      "10  secretary  junior  21-30     Low\n",
      "\n",
      "Dataset shape: (11, 4)\n",
      "Total records: 11\n"
     ]
    }
   ],
   "source": [
    "# Create the employee dataset\n",
    "data = {\n",
    "    'department': ['sales', 'sales', 'sales', 'systems', 'systems', 'systems', 'systems', 'marketing', 'marketing', 'secretary', 'secretary'],\n",
    "    'status': ['senior', 'junior', 'junior', 'junior', 'senior', 'junior', 'senior', 'senior', 'junior', 'senior', 'junior'],\n",
    "    'age': ['31-40', '21-30', '31-40', '21-30', '31-40', '21-30', '41-50', '31-40', '31-40', '41-50', '21-30'],\n",
    "    'salary': ['Medium', 'Low', 'Low', 'Medium', 'High', 'Medium', 'High', 'Medium', 'Medium', 'Medium', 'Low']\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Total records: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7032228c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STEP 1: Calculate Entropy of Target Attribute (Salary) ===\n",
      "Salary class distribution: Counter({'Medium': 6, 'Low': 3, 'High': 2})\n",
      "Total samples: 11\n",
      "\n",
      "Detailed calculation:\n",
      "P(Medium) = 6/11 = 0.5455\n",
      "  -0.5455 * log2(0.5455) = 0.4770\n",
      "P(Low) = 3/11 = 0.2727\n",
      "  -0.2727 * log2(0.2727) = 0.5112\n",
      "P(High) = 2/11 = 0.1818\n",
      "  -0.1818 * log2(0.1818) = 0.4472\n",
      "\n",
      "Total Entropy = 1.4354\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Calculate entropy of the target attribute (salary)\n",
    "def calculate_entropy(data, target_column):\n",
    "    \"\"\"\n",
    "    Calculate entropy of the target attribute\n",
    "    Entropy = -Σ(p_i * log2(p_i)) where p_i is the proportion of class i\n",
    "    \"\"\"\n",
    "    # Count frequency of each class\n",
    "    class_counts = Counter(data[target_column])\n",
    "    total_samples = len(data)\n",
    "    \n",
    "    entropy = 0\n",
    "    for count in class_counts.values():\n",
    "        probability = count / total_samples\n",
    "        if probability > 0:  # Avoid log(0)\n",
    "            entropy -= probability * math.log2(probability)\n",
    "    \n",
    "    return entropy, class_counts\n",
    "\n",
    "# Calculate entropy of salary (target attribute)\n",
    "salary_entropy, salary_counts = calculate_entropy(df, 'salary')\n",
    "print(\"=== STEP 1: Calculate Entropy of Target Attribute (Salary) ===\")\n",
    "print(f\"Salary class distribution: {salary_counts}\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "\n",
    "# Display the calculation step by step\n",
    "print(\"\\nDetailed calculation:\")\n",
    "for salary_class, count in salary_counts.items():\n",
    "    probability = count / len(df)\n",
    "    log_prob = math.log2(probability) if probability > 0 else 0\n",
    "    contribution = -probability * log_prob\n",
    "    print(f\"P({salary_class}) = {count}/{len(df)} = {probability:.4f}\")\n",
    "    print(f\"  -{probability:.4f} * log2({probability:.4f}) = {contribution:.4f}\")\n",
    "\n",
    "print(f\"\\nTotal Entropy = {salary_entropy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75ffc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STEP 2: Calculate Information Gain for Each Attribute ===\n",
      "\n",
      "--- Information Gain for department ---\n",
      "  department = sales: 3/11 samples\n",
      "    Subset entropy: 0.9183\n",
      "    Weight: 0.2727\n",
      "    Weighted contribution: 0.2504\n",
      "    Class distribution: {'Medium': 1, 'Low': 2}\n",
      "  department = systems: 4/11 samples\n",
      "    Subset entropy: 1.0000\n",
      "    Weight: 0.3636\n",
      "    Weighted contribution: 0.3636\n",
      "    Class distribution: {'Medium': 2, 'High': 2}\n",
      "  department = marketing: 2/11 samples\n",
      "    Subset entropy: 0.0000\n",
      "    Weight: 0.1818\n",
      "    Weighted contribution: 0.0000\n",
      "    Class distribution: {'Medium': 2}\n",
      "  department = secretary: 2/11 samples\n",
      "    Subset entropy: 1.0000\n",
      "    Weight: 0.1818\n",
      "    Weighted contribution: 0.1818\n",
      "    Class distribution: {'Medium': 1, 'Low': 1}\n",
      "  Weighted entropy: 0.7959\n",
      "  Information Gain: 1.4354 - 0.7959 = 0.6395\n",
      "\n",
      "--- Information Gain for status ---\n",
      "  status = senior: 5/11 samples\n",
      "    Subset entropy: 0.9710\n",
      "    Weight: 0.4545\n",
      "    Weighted contribution: 0.4413\n",
      "    Class distribution: {'Medium': 3, 'High': 2}\n",
      "  status = junior: 6/11 samples\n",
      "    Subset entropy: 1.0000\n",
      "    Weight: 0.5455\n",
      "    Weighted contribution: 0.5455\n",
      "    Class distribution: {'Low': 3, 'Medium': 3}\n",
      "  Weighted entropy: 0.9868\n",
      "  Information Gain: 1.4354 - 0.9868 = 0.4486\n",
      "\n",
      "--- Information Gain for age ---\n",
      "  age = 31-40: 5/11 samples\n",
      "    Subset entropy: 1.3710\n",
      "    Weight: 0.4545\n",
      "    Weighted contribution: 0.6232\n",
      "    Class distribution: {'Medium': 3, 'Low': 1, 'High': 1}\n",
      "  age = 21-30: 4/11 samples\n",
      "    Subset entropy: 1.0000\n",
      "    Weight: 0.3636\n",
      "    Weighted contribution: 0.3636\n",
      "    Class distribution: {'Low': 2, 'Medium': 2}\n",
      "  age = 41-50: 2/11 samples\n",
      "    Subset entropy: 1.0000\n",
      "    Weight: 0.1818\n",
      "    Weighted contribution: 0.1818\n",
      "    Class distribution: {'High': 1, 'Medium': 1}\n",
      "  Weighted entropy: 1.1686\n",
      "  Information Gain: 1.4354 - 1.1686 = 0.2668\n",
      "\n",
      "Summary of Information Gains:\n",
      "  department: 0.6395\n",
      "  status: 0.4486\n",
      "  age: 0.2668\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Calculate Information Gain for each attribute\n",
    "def calculate_information_gain(data, attribute, target_column):\n",
    "    \"\"\"\n",
    "    Calculate information gain for a given attribute\n",
    "    Information Gain = Entropy(S) - Σ(|Sv|/|S|) * Entropy(Sv)\n",
    "    where Sv is the subset of S for which attribute A has value v\n",
    "    \"\"\"\n",
    "    # Get unique values of the attribute\n",
    "    attribute_values = data[attribute].unique()\n",
    "    total_samples = len(data)\n",
    "    \n",
    "    # Calculate weighted entropy for each attribute value\n",
    "    weighted_entropy = 0\n",
    "    print(f\"\\n--- Information Gain for {attribute} ---\")\n",
    "    \n",
    "    for value in attribute_values:\n",
    "        # Get subset of data where attribute = value\n",
    "        subset = data[data[attribute] == value]\n",
    "        subset_size = len(subset)\n",
    "        \n",
    "        # Calculate entropy for this subset\n",
    "        subset_entropy, subset_counts = calculate_entropy(subset, target_column)\n",
    "        \n",
    "        # Weight by proportion of samples\n",
    "        weight = subset_size / total_samples\n",
    "        weighted_entropy += weight * subset_entropy\n",
    "        \n",
    "        print(f\"  {attribute} = {value}: {subset_size}/{total_samples} samples\")\n",
    "        print(f\"    Subset entropy: {subset_entropy:.4f}\")\n",
    "        print(f\"    Weight: {weight:.4f}\")\n",
    "        print(f\"    Weighted contribution: {weight * subset_entropy:.4f}\")\n",
    "        print(f\"    Class distribution: {dict(subset_counts)}\")\n",
    "    \n",
    "    # Information Gain = Original Entropy - Weighted Entropy\n",
    "    information_gain = salary_entropy - weighted_entropy\n",
    "    print(f\"  Weighted entropy: {weighted_entropy:.4f}\")\n",
    "    print(f\"  Information Gain: {salary_entropy:.4f} - {weighted_entropy:.4f} = {information_gain:.4f}\")\n",
    "    \n",
    "    return information_gain, weighted_entropy\n",
    "\n",
    "attributes = ['department', 'status', 'age']\n",
    "information_gains = {}\n",
    "weighted_entropies = {}\n",
    "\n",
    "print(\"=== STEP 2: Calculate Information Gain for Each Attribute ===\")\n",
    "for attr in attributes:\n",
    "    ig, we = calculate_information_gain(df, attr, 'salary')\n",
    "    information_gains[attr] = ig\n",
    "    weighted_entropies[attr] = we\n",
    "\n",
    "print(f\"\\nSummary of Information Gains:\")\n",
    "for attr, ig in information_gains.items():\n",
    "    print(f\"  {attr}: {ig:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "943531b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STEP 3: Calculate Split Information for Each Attribute ===\n",
      "\n",
      "--- Split Information for department ---\n",
      "  department = sales: 3/11 = 0.2727\n",
      "    -0.2727 * log2(0.2727) = 0.5112\n",
      "  department = systems: 4/11 = 0.3636\n",
      "    -0.3636 * log2(0.3636) = 0.5307\n",
      "  department = marketing: 2/11 = 0.1818\n",
      "    -0.1818 * log2(0.1818) = 0.4472\n",
      "  department = secretary: 2/11 = 0.1818\n",
      "    -0.1818 * log2(0.1818) = 0.4472\n",
      "  Split Information: 1.9363\n",
      "\n",
      "--- Split Information for status ---\n",
      "  status = senior: 5/11 = 0.4545\n",
      "    -0.4545 * log2(0.4545) = 0.5170\n",
      "  status = junior: 6/11 = 0.5455\n",
      "    -0.5455 * log2(0.5455) = 0.4770\n",
      "  Split Information: 0.9940\n",
      "\n",
      "--- Split Information for age ---\n",
      "  age = 31-40: 5/11 = 0.4545\n",
      "    -0.4545 * log2(0.4545) = 0.5170\n",
      "  age = 21-30: 4/11 = 0.3636\n",
      "    -0.3636 * log2(0.3636) = 0.5307\n",
      "  age = 41-50: 2/11 = 0.1818\n",
      "    -0.1818 * log2(0.1818) = 0.4472\n",
      "  Split Information: 1.4949\n",
      "\n",
      "Summary of Split Information:\n",
      "  department: 1.9363\n",
      "  status: 0.9940\n",
      "  age: 1.4949\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Calculate Split Information for each attribute\n",
    "def calculate_split_information(data, attribute):\n",
    "    \"\"\"\n",
    "    Calculate split information for a given attribute\n",
    "    Split Information = -Σ(|Si|/|S|) * log2(|Si|/|S|)\n",
    "    where Si is the subset of S for which attribute A has value i\n",
    "    \"\"\"\n",
    "    # Get unique values of the attribute\n",
    "    attribute_values = data[attribute].unique()\n",
    "    total_samples = len(data)\n",
    "    \n",
    "    split_info = 0\n",
    "    print(f\"\\n--- Split Information for {attribute} ---\")\n",
    "    \n",
    "    for value in attribute_values:\n",
    "        # Count samples for this attribute value\n",
    "        subset_size = len(data[data[attribute] == value])\n",
    "        proportion = subset_size / total_samples\n",
    "        \n",
    "        if proportion > 0:  # Avoid log(0)\n",
    "            split_info -= proportion * math.log2(proportion)\n",
    "            print(f\"  {attribute} = {value}: {subset_size}/{total_samples} = {proportion:.4f}\")\n",
    "            print(f\"    -{proportion:.4f} * log2({proportion:.4f}) = {-proportion * math.log2(proportion):.4f}\")\n",
    "    \n",
    "    print(f\"  Split Information: {split_info:.4f}\")\n",
    "    return split_info\n",
    "\n",
    "# Calculate split information for each attribute\n",
    "split_informations = {}\n",
    "\n",
    "print(\"=== STEP 3: Calculate Split Information for Each Attribute ===\")\n",
    "for attr in attributes:\n",
    "    si = calculate_split_information(df, attr)\n",
    "    split_informations[attr] = si\n",
    "\n",
    "print(f\"\\nSummary of Split Information:\")\n",
    "for attr, si in split_informations.items():\n",
    "    print(f\"  {attr}: {si:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33af121d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STEP 4: Calculate Gain Ratio for Each Attribute ===\n",
      "Gain Ratio = Information Gain / Split Information\n",
      "\n",
      "department:\n",
      "  Information Gain: 0.6395\n",
      "  Split Information: 1.9363\n",
      "  Gain Ratio: 0.6395 / 1.9363 = 0.3303\n",
      "\n",
      "status:\n",
      "  Information Gain: 0.4486\n",
      "  Split Information: 0.9940\n",
      "  Gain Ratio: 0.4486 / 0.9940 = 0.4513\n",
      "\n",
      "age:\n",
      "  Information Gain: 0.2668\n",
      "  Split Information: 1.4949\n",
      "  Gain Ratio: 0.2668 / 1.4949 = 0.1784\n",
      "\n",
      "Summary of Gain Ratios:\n",
      "  department: 0.3303\n",
      "  status: 0.4513\n",
      "  age: 0.1784\n",
      "\n",
      "RESULTS:\n",
      "==================================================\n",
      "Attribute with highest Gain Ratio: status\n",
      "Gain Ratio: 0.4513\n",
      "status attribute should be selected to split the records in the first iteration: status\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Calculate Gain Ratio for each attribute\n",
    "def calculate_gain_ratio(information_gain, split_information):\n",
    "    \"\"\"\n",
    "    Calculate gain ratio for an attribute\n",
    "    Gain Ratio = Information Gain / Split Information\n",
    "    \"\"\"\n",
    "    if split_information == 0:\n",
    "        return 0  # Avoid division by zero\n",
    "    return information_gain / split_information\n",
    "\n",
    "# Calculate gain ratio for each attribute\n",
    "gain_ratios = {}\n",
    "\n",
    "print(\"=== STEP 4: Calculate Gain Ratio for Each Attribute ===\")\n",
    "print(\"Gain Ratio = Information Gain / Split Information\")\n",
    "print()\n",
    "\n",
    "for attr in attributes:\n",
    "    ig = information_gains[attr]\n",
    "    si = split_informations[attr]\n",
    "    gr = calculate_gain_ratio(ig, si)\n",
    "    gain_ratios[attr] = gr\n",
    "    \n",
    "    print(f\"{attr}:\")\n",
    "    print(f\"  Information Gain: {ig:.4f}\")\n",
    "    print(f\"  Split Information: {si:.4f}\")\n",
    "    print(f\"  Gain Ratio: {ig:.4f} / {si:.4f} = {gr:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(\"Summary of Gain Ratios:\")\n",
    "for attr, gr in gain_ratios.items():\n",
    "    print(f\"  {attr}: {gr:.4f}\")\n",
    "\n",
    "# Find the attribute with the highest gain ratio\n",
    "best_attribute = max(gain_ratios, key=gain_ratios.get)\n",
    "best_gain_ratio = gain_ratios[best_attribute]\n",
    "print()\n",
    "print(\"RESULTS:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Attribute with highest Gain Ratio: {best_attribute}\")\n",
    "print(f\"Gain Ratio: {best_gain_ratio:.4f}\")\n",
    "print(f\"{best_attribute} attribute should be selected to split the records in the first iteration: {best_attribute}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdba728",
   "metadata": {},
   "source": [
    "## Summary and Conclusion\n",
    "\n",
    "### C4.5 Decision Tree Algorithm Analysis\n",
    "\n",
    "The C4.5 algorithm uses **Gain Ratio** as the splitting criterion to overcome the bias of Information Gain toward attributes with many values. The Gain Ratio is calculated as:\n",
    "\n",
    "**Gain Ratio = Information Gain / Split Information**\n",
    "\n",
    "### Final Answer:\n",
    "The attribute ''Status'' should be selected for the first split in the C4.5 decision tree algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0e4a31",
   "metadata": {},
   "source": [
    "# 2. Naive Bayesian Classifier for Tennis Playing Decision\n",
    "\n",
    "## Problem Statement\n",
    "Given the training dataset for tennis playing decisions based on weather conditions, we need to:\n",
    "1. Build a Naive Bayesian Classifier (NBC) based on the training data\n",
    "2. Use the classifier to predict whether to play tennis when Outlook=sunny, Temperature=cool, Humidity=high, and Windy=True\n",
    "\n",
    "## Training Dataset\n",
    "The dataset contains 14 records with attributes:\n",
    "- **Outlook**: sunny, overcast, rainy\n",
    "- **Temperature**: hot, mild, cool  \n",
    "- **Humidity**: high, normal\n",
    "- **Windy**: TRUE, FALSE\n",
    "- **Play** (class label): yes, no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70133658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TENNIS PLAYING DATASET ===\n",
      "     Outlook Temperature Humidity  Windy Play\n",
      "0      sunny         hot     high  False   no\n",
      "1      sunny         hot     high   True   no\n",
      "2   overcast         hot     high  False  yes\n",
      "3      rainy        mild     high  False  yes\n",
      "4      rainy        cool   normal  False  yes\n",
      "5      rainy        cool   normal   True   no\n",
      "6   overcast        cool   normal   True  yes\n",
      "7      sunny        mild     high  False   no\n",
      "8      sunny        cool   normal  False  yes\n",
      "9      rainy        mild   normal  False  yes\n",
      "10     sunny        mild   normal   True  yes\n",
      "11  overcast        mild     high   True  yes\n",
      "12  overcast         hot   normal  False  yes\n",
      "13     rainy        mild     high   True   no\n",
      "\n",
      "Dataset shape: (14, 5)\n",
      "Total records: 14\n",
      "\n",
      "Class distribution:\n",
      "  no: 5 (35.7%)\n",
      "  yes: 9 (64.3%)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for Naive Bayesian Classifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "\n",
    "# Set up the tennis playing dataset\n",
    "tennis_data = {\n",
    "    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast', \n",
    "                'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy'],\n",
    "    'Temperature': ['hot', 'hot', 'hot', 'mild', 'cool', 'cool', 'cool', \n",
    "                    'mild', 'cool', 'mild', 'mild', 'mild', 'hot', 'mild'],\n",
    "    'Humidity': ['high', 'high', 'high', 'high', 'normal', 'normal', 'normal', \n",
    "                 'high', 'normal', 'normal', 'normal', 'high', 'normal', 'high'],\n",
    "    'Windy': [False, True, False, False, False, True, True, \n",
    "              False, False, False, True, True, False, True],\n",
    "    'Play': ['no', 'no', 'yes', 'yes', 'yes', 'no', 'yes', \n",
    "             'no', 'yes', 'yes', 'yes', 'yes', 'yes', 'no']\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df_tennis = pd.DataFrame(tennis_data)\n",
    "print(\"=== TENNIS PLAYING DATASET ===\")\n",
    "print(df_tennis)\n",
    "print(f\"\\nDataset shape: {df_tennis.shape}\")\n",
    "print(f\"Total records: {len(df_tennis)}\")\n",
    "\n",
    "# Display class distribution\n",
    "play_counts = Counter(df_tennis['Play'])\n",
    "print(f\"\\nClass distribution:\")\n",
    "for play_class, count in play_counts.items():\n",
    "    print(f\"  {play_class}: {count} ({count/len(df_tennis)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a485f2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PRIOR PROBABILITIES P(Play) ===\n",
      "P(Play = no) = 5/14 = 0.3571\n",
      "P(Play = yes) = 9/14 = 0.6429\n",
      "\n",
      "=== LIKELIHOOD PROBABILITIES P(Attribute|Class) ===\n",
      "\n",
      "--- Outlook ---\n",
      "\n",
      "  Given Play = no (n = 5):\n",
      "    P(Outlook = sunny | Play = no) = 3/5 = 0.6000\n",
      "    P(Outlook = overcast | Play = no) = 0/5 = 0.0000\n",
      "    P(Outlook = rainy | Play = no) = 2/5 = 0.4000\n",
      "\n",
      "  Given Play = yes (n = 9):\n",
      "    P(Outlook = sunny | Play = yes) = 2/9 = 0.2222\n",
      "    P(Outlook = overcast | Play = yes) = 4/9 = 0.4444\n",
      "    P(Outlook = rainy | Play = yes) = 3/9 = 0.3333\n",
      "\n",
      "--- Temperature ---\n",
      "\n",
      "  Given Play = no (n = 5):\n",
      "    P(Temperature = hot | Play = no) = 2/5 = 0.4000\n",
      "    P(Temperature = mild | Play = no) = 2/5 = 0.4000\n",
      "    P(Temperature = cool | Play = no) = 1/5 = 0.2000\n",
      "\n",
      "  Given Play = yes (n = 9):\n",
      "    P(Temperature = hot | Play = yes) = 2/9 = 0.2222\n",
      "    P(Temperature = mild | Play = yes) = 4/9 = 0.4444\n",
      "    P(Temperature = cool | Play = yes) = 3/9 = 0.3333\n",
      "\n",
      "--- Humidity ---\n",
      "\n",
      "  Given Play = no (n = 5):\n",
      "    P(Humidity = high | Play = no) = 4/5 = 0.8000\n",
      "    P(Humidity = normal | Play = no) = 1/5 = 0.2000\n",
      "\n",
      "  Given Play = yes (n = 9):\n",
      "    P(Humidity = high | Play = yes) = 3/9 = 0.3333\n",
      "    P(Humidity = normal | Play = yes) = 6/9 = 0.6667\n",
      "\n",
      "--- Windy ---\n",
      "\n",
      "  Given Play = no (n = 5):\n",
      "    P(Windy = False | Play = no) = 2/5 = 0.4000\n",
      "    P(Windy = True | Play = no) = 3/5 = 0.6000\n",
      "\n",
      "  Given Play = yes (n = 9):\n",
      "    P(Windy = False | Play = yes) = 6/9 = 0.6667\n",
      "    P(Windy = True | Play = yes) = 3/9 = 0.3333\n",
      "\n",
      "=== SUMMARY OF ALL PROBABILITIES ===\n",
      "Prior probabilities: {'no': 0.35714285714285715, 'yes': 0.6428571428571429}\n",
      "Likelihood probabilities calculated for attributes: ['Outlook', 'Temperature', 'Humidity', 'Windy']\n"
     ]
    }
   ],
   "source": [
    "# Part (a): Build Naive Bayesian Classifier\n",
    "# Calculate all probabilities required by the classifier\n",
    "\n",
    "def calculate_prior_probabilities(data, target_column):\n",
    "    \"\"\"\n",
    "    Calculate prior probabilities P(C) for each class\n",
    "    \"\"\"\n",
    "    class_counts = Counter(data[target_column])\n",
    "    total_samples = len(data)\n",
    "    \n",
    "    prior_probs = {}\n",
    "    print(\"=== PRIOR PROBABILITIES P(Play) ===\")\n",
    "    for class_name, count in class_counts.items():\n",
    "        prob = count / total_samples\n",
    "        prior_probs[class_name] = prob\n",
    "        print(f\"P(Play = {class_name}) = {count}/{total_samples} = {prob:.4f}\")\n",
    "    \n",
    "    return prior_probs\n",
    "\n",
    "def calculate_likelihood_probabilities(data, target_column, attributes):\n",
    "    \"\"\"\n",
    "    Calculate likelihood probabilities P(Ai|C) for each attribute given each class\n",
    "    \"\"\"\n",
    "    likelihood_probs = {}\n",
    "    class_counts = Counter(data[target_column])\n",
    "    \n",
    "    print(\"\\n=== LIKELIHOOD PROBABILITIES P(Attribute|Class) ===\")\n",
    "    \n",
    "    for attr in attributes:\n",
    "        likelihood_probs[attr] = {}\n",
    "        print(f\"\\n--- {attr} ---\")\n",
    "        \n",
    "        # Get unique values for this attribute\n",
    "        attr_values = data[attr].unique()\n",
    "        \n",
    "        for class_name in class_counts.keys():\n",
    "            likelihood_probs[attr][class_name] = {}\n",
    "            \n",
    "            # Filter data for this class\n",
    "            class_data = data[data[target_column] == class_name]\n",
    "            class_size = len(class_data)\n",
    "            \n",
    "            print(f\"\\n  Given Play = {class_name} (n = {class_size}):\")\n",
    "            \n",
    "            for attr_value in attr_values:\n",
    "                # Count occurrences of this attribute value in this class\n",
    "                count = len(class_data[class_data[attr] == attr_value])\n",
    "                prob = count / class_size if class_size > 0 else 0\n",
    "                likelihood_probs[attr][class_name][attr_value] = prob\n",
    "                \n",
    "                print(f\"    P({attr} = {attr_value} | Play = {class_name}) = {count}/{class_size} = {prob:.4f}\")\n",
    "    \n",
    "    return likelihood_probs\n",
    "\n",
    "# Calculate all probabilities\n",
    "target_column = 'Play'\n",
    "attributes = ['Outlook', 'Temperature', 'Humidity', 'Windy']\n",
    "\n",
    "# Calculate prior probabilities\n",
    "prior_probs = calculate_prior_probabilities(df_tennis, target_column)\n",
    "\n",
    "# Calculate likelihood probabilities\n",
    "likelihood_probs = calculate_likelihood_probabilities(df_tennis, target_column, attributes)\n",
    "\n",
    "print(f\"\\n=== SUMMARY OF ALL PROBABILITIES ===\")\n",
    "print(f\"Prior probabilities: {prior_probs}\")\n",
    "print(f\"Likelihood probabilities calculated for attributes: {attributes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30d97fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NAIVE BAYESIAN PREDICTION ===\n",
      "Test instance: {'Outlook': 'sunny', 'Temperature': 'cool', 'Humidity': 'high', 'Windy': True}\n",
      "\n",
      "--- Calculating P(Play = no | X) ---\n",
      "P(Play = no) = 0.3571\n",
      "P(Outlook = sunny | Play = no) = 0.6000\n",
      "  Updated posterior: 0.214286\n",
      "P(Temperature = cool | Play = no) = 0.2000\n",
      "  Updated posterior: 0.042857\n",
      "P(Humidity = high | Play = no) = 0.8000\n",
      "  Updated posterior: 0.034286\n",
      "P(Windy = True | Play = no) = 0.6000\n",
      "  Updated posterior: 0.020571\n",
      "Final P(Play = no | X) = 0.020571\n",
      "\n",
      "--- Calculating P(Play = yes | X) ---\n",
      "P(Play = yes) = 0.6429\n",
      "P(Outlook = sunny | Play = yes) = 0.2222\n",
      "  Updated posterior: 0.142857\n",
      "P(Temperature = cool | Play = yes) = 0.3333\n",
      "  Updated posterior: 0.047619\n",
      "P(Humidity = high | Play = yes) = 0.3333\n",
      "  Updated posterior: 0.015873\n",
      "P(Windy = True | Play = yes) = 0.3333\n",
      "  Updated posterior: 0.005291\n",
      "Final P(Play = yes | X) = 0.005291\n",
      "\n",
      "=== PREDICTION RESULT ===\n",
      "Posterior probabilities:\n",
      "  P(Play = no | X) = 0.020571\n",
      "  P(Play = yes | X) = 0.005291\n",
      "\n",
      "Predicted class: no\n",
      "Confidence: 0.020571\n"
     ]
    }
   ],
   "source": [
    "# Part (b): Use the classifier to make predictions\n",
    "# Test case: Outlook=sunny, Temperature=cool, Humidity=high, Windy=True\n",
    "\n",
    "def naive_bayes_predict(instance, prior_probs, likelihood_probs, attributes):\n",
    "    \"\"\"\n",
    "    Make prediction using Naive Bayesian Classifier\n",
    "    P(C|X) ∝ P(C) * ∏ P(Ai|C)\n",
    "    \"\"\"\n",
    "    print(\"=== NAIVE BAYESIAN PREDICTION ===\")\n",
    "    print(f\"Test instance: {instance}\")\n",
    "    print()\n",
    "    \n",
    "    # Calculate posterior probabilities for each class\n",
    "    posterior_probs = {}\n",
    "    \n",
    "    for class_name in prior_probs.keys():\n",
    "        print(f\"--- Calculating P(Play = {class_name} | X) ---\")\n",
    "        \n",
    "        # Start with prior probability\n",
    "        posterior = prior_probs[class_name]\n",
    "        print(f\"P(Play = {class_name}) = {posterior:.4f}\")\n",
    "        \n",
    "        # Multiply by likelihood for each attribute\n",
    "        for attr in attributes:\n",
    "            attr_value = instance[attr]\n",
    "            likelihood = likelihood_probs[attr][class_name][attr_value]\n",
    "            posterior *= likelihood\n",
    "            \n",
    "            print(f\"P({attr} = {attr_value} | Play = {class_name}) = {likelihood:.4f}\")\n",
    "            print(f\"  Updated posterior: {posterior:.6f}\")\n",
    "        \n",
    "        posterior_probs[class_name] = posterior\n",
    "        print(f\"Final P(Play = {class_name} | X) = {posterior:.6f}\")\n",
    "        print()\n",
    "    \n",
    "    # Find the class with highest posterior probability\n",
    "    predicted_class = max(posterior_probs, key=posterior_probs.get)\n",
    "    \n",
    "    print(\"=== PREDICTION RESULT ===\")\n",
    "    print(f\"Posterior probabilities:\")\n",
    "    for class_name, prob in posterior_probs.items():\n",
    "        print(f\"  P(Play = {class_name} | X) = {prob:.6f}\")\n",
    "    \n",
    "    print(f\"\\nPredicted class: {predicted_class}\")\n",
    "    print(f\"Confidence: {posterior_probs[predicted_class]:.6f}\")\n",
    "    \n",
    "    return predicted_class, posterior_probs\n",
    "\n",
    "# Test instance: Outlook=sunny, Temperature=cool, Humidity=high, Windy=True\n",
    "test_instance = {\n",
    "    'Outlook': 'sunny',\n",
    "    'Temperature': 'cool', \n",
    "    'Humidity': 'high',\n",
    "    'Windy': True\n",
    "}\n",
    "\n",
    "# Make prediction\n",
    "predicted_class, posterior_probs = naive_bayes_predict(test_instance, prior_probs, likelihood_probs, attributes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5907b951",
   "metadata": {},
   "source": [
    "## Summary and Conclusion\n",
    "\n",
    "### Naive Bayesian Classifier Implementation\n",
    "\n",
    "The Naive Bayesian Classifier was successfully implemented with the following key components: **Predicted Class - No.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9482c8a3",
   "metadata": {},
   "source": [
    "# Independence Analysis: Humidity and Windy Attributes\n",
    "\n",
    "## Problem Statement\n",
    "We need to analyze the independence relationships between Humidity and Windy attributes:\n",
    "- **(c) Marginal Independence**: Are Humidity and Windy independent?\n",
    "- **(d) Conditional Independence**: Are Humidity and Windy conditionally independent given the class label Play?\n",
    "\n",
    "## Statistical Framework\n",
    "For two attributes A and B to be independent:\n",
    "- **Marginal Independence**: P(A, B) = P(A) × P(B)\n",
    "- **Conditional Independence**: P(A, B | C) = P(A | C) × P(B | C)\n",
    "\n",
    "We'll use chi-square tests and probability analysis to determine independence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5839566b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Windy</th>\n",
       "      <th>Expected</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Match?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>high</td>\n",
       "      <td>False</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.286</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>high</td>\n",
       "      <td>True</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.214</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>normal</td>\n",
       "      <td>False</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.286</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>normal</td>\n",
       "      <td>True</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.214</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Humidity  Windy  Expected  Actual  Match?\n",
       "0     high  False     0.286   0.286    True\n",
       "1     high   True     0.214   0.214    True\n",
       "2   normal  False     0.286   0.286    True\n",
       "3   normal   True     0.214   0.214    True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part (c): Marginal Independence Analysis\n",
    "# Are Humidity and Windy independent?\n",
    "\n",
    "# Marginal probabilities\n",
    "p_humidity = df_tennis[\"Humidity\"].value_counts(normalize=True)\n",
    "p_windy = df_tennis[\"Windy\"].value_counts(normalize=True)\n",
    "\n",
    "# Joint probabilities\n",
    "p_joint = df_tennis.groupby([\"Humidity\",\"Windy\"]).size() / len(df_tennis)\n",
    "\n",
    "# Compare expected vs actual\n",
    "results = []\n",
    "for h in p_humidity.index:\n",
    "    for w in p_windy.index:\n",
    "        expected = p_humidity[h] * p_windy[w]\n",
    "        actual = p_joint.loc[h, w]\n",
    "        results.append({\n",
    "            \"Humidity\": h,\n",
    "            \"Windy\": w,\n",
    "            \"Expected\": round(expected, 3),\n",
    "            \"Actual\": round(actual, 3),\n",
    "            \"Match?\": abs(expected - actual) < 1e-6\n",
    "        })\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e9358f",
   "metadata": {},
   "source": [
    "## Answer\n",
    "Since in all cases actual = expected, the attributes Humidity and Windy are independent in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "204c317d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Play = no\n",
      "('high', False): {'P_joint': 0.4, 'P_expected': 0.32, 'Match': False}\n",
      "('high', True): {'P_joint': 0.4, 'P_expected': 0.48, 'Match': False}\n",
      "('normal', True): {'P_joint': 0.2, 'P_expected': 0.12, 'Match': False}\n",
      "\n",
      "Play = yes\n",
      "('high', False): {'P_joint': 0.222, 'P_expected': 0.222, 'Match': True}\n",
      "('high', True): {'P_joint': 0.111, 'P_expected': 0.111, 'Match': True}\n",
      "('normal', False): {'P_joint': 0.444, 'P_expected': 0.444, 'Match': True}\n",
      "('normal', True): {'P_joint': 0.222, 'P_expected': 0.222, 'Match': True}\n"
     ]
    }
   ],
   "source": [
    "# Part (d): Conditional Independence Analysis\n",
    "# Are Humidity and Windy conditionally independent given Play class?\n",
    "\n",
    "# Function to check conditional independence\n",
    "def check_conditional_independence(df, attr1, attr2, given):\n",
    "    results = {}\n",
    "    for val in df[given].unique():\n",
    "        subset = df[df[given] == val]\n",
    "        total = len(subset)\n",
    "        \n",
    "        # Marginal probabilities\n",
    "        p_attr1 = subset[attr1].value_counts(normalize=True).to_dict()\n",
    "        p_attr2 = subset[attr2].value_counts(normalize=True).to_dict()\n",
    "        \n",
    "        # Joint probabilities\n",
    "        joint = subset.groupby([attr1, attr2]).size() / total\n",
    "        \n",
    "        # Compare P(attr1, attr2 | given) vs P(attr1|given) * P(attr2|given)\n",
    "        comparison = {}\n",
    "        for (a1, a2), p_joint in joint.items():\n",
    "            p_expected = p_attr1.get(a1, 0) * p_attr2.get(a2, 0)\n",
    "            comparison[(a1, a2)] = {\n",
    "                \"P_joint\": round(p_joint, 3),\n",
    "                \"P_expected\": round(p_expected, 3),\n",
    "                \"Match\": abs(p_joint - p_expected) < 1e-6\n",
    "            }\n",
    "        results[val] = comparison\n",
    "    return results\n",
    "\n",
    "results = check_conditional_independence(df_tennis, \"Humidity\", \"Windy\", \"Play\")\n",
    "\n",
    "# Print results\n",
    "for play_val, comp in results.items():\n",
    "    print(f\"\\nPlay = {play_val}\")\n",
    "    for cond, vals in comp.items():\n",
    "        print(f\"{cond}: {vals}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93900f4",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "For Play = yes → all Match = True → independent.\n",
    "For Play = no → Match = False → not independent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d311306",
   "metadata": {},
   "source": [
    "## 3. Efficient Modification for Generalized Data Records\n",
    "\n",
    "**Key Modifications:**\n",
    "\n",
    "1. **Weighted Entropy Calculation**: Replace simple counts with weighted sums\n",
    "   - `H(S) = -Σ(w_i/W) * log2(w_i/W)` where `w_i` is the weight (count) of class i and `W` is total weight\n",
    "\n",
    "2. **Weighted Information Gain**: Use weighted proportions in entropy calculations\n",
    "   - `IG(S,A) = H(S) - Σ(W_v/W) * H(S_v)` where `W_v` is total weight for attribute value v\n",
    "\n",
    "3. **Weighted Split Information**: Calculate using weighted counts\n",
    "   - `SI(S,A) = -Σ(W_i/W) * log2(W_i/W)` where `W_i` is total weight for attribute value i\n",
    "\n",
    "**Efficiency Benefits:**\n",
    "- **Space Complexity**: O(n) instead of O(Σcount_i) where n is unique records\n",
    "- **Memory Usage**: 93.3% reduction (165 records → 11 unique records)\n",
    "- **Time Complexity**: O(n) instead of O(Σcount_i) for calculations\n",
    "\n",
    "**Implementation Strategy:**\n",
    "- Preserve count attribute throughout all calculations\n",
    "- Use weighted sums instead of simple counts\n",
    "- Maintain mathematical correctness with weighted proportions\n",
    "- Validate results by comparing with expanded unweighted data\n",
    "\n",
    "**Example Efficiency:**\n",
    "- Original approach: Would need to expand 165 records (sum of all counts)\n",
    "- Weighted approach: Only processes 11 unique records\n",
    "- **Result**: Identical mathematical results with dramatically better computational efficiency\n",
    "\n",
    "This approach maintains the mathematical integrity of the C4.5 algorithm while providing significant performance improvements for generalized data records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e947ad78",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
